---
description: 
globs: 
---

use these models:
- o3-mini (good for reasoning)
- claude-3-5-sonnet-20241022 (pdf, extraction really good)		



# Vercel AI SDK Integration Guide

## Overview
The Vercel AI SDK provides a powerful framework for building AI-powered applications with a focus on streaming responses, tool usage, and agent-based architectures. This guide covers key concepts and implementation patterns.

## Core Components

### 1. Building Blocks

#### Single-Step LLM Generation
- Basic text generation with one LLM call
- Useful for classification, simple text generation
- Direct response without additional processing

#### Tool Usage
- Extends LLM capabilities through function calling
- Tools can be APIs, databases, or utility functions
- Supports multi-step execution with `maxSteps`

#### Multi-Agent Systems
- Multiple LLMs working together
- Specialized roles for complex tasks
- Coordinated workflow execution

### 2. Implementation Patterns

#### Sequential Processing (Chains)
```typescript
async function processContent(input: string) {
  const model = openai('gpt-4o');
  
  // First step: Generate content
  const { text: content } = await generateText({
    model,
    prompt: `Process this input: ${input}`,
  });

  // Second step: Quality check
  const { object: quality } = await generateObject({
    model,
    schema: z.object({
      score: z.number().min(1).max(10),
      improvements: z.array(z.string()),
    }),
    prompt: `Evaluate this content: ${content}`,
  });

  return { content, quality };
}
```

#### Routing Pattern
```typescript
async function handleRequest(input: string) {
  const { object: classification } = await generateObject({
    model,
    schema: z.object({
      type: z.enum(['general', 'technical', 'support']),
      priority: z.enum(['low', 'medium', 'high']),
    }),
    prompt: `Classify this request: ${input}`,
  });

  // Route based on classification
  const response = await generateText({
    model: classification.priority === 'high' 
      ? openai('gpt-4o')
      : openai('gpt-4o-mini'),
    system: getSystemPrompt(classification.type),
    prompt: input,
  });

  return response;
}
```

### 3. Tool Integration

#### Defining Tools
```typescript
const tools = {
  calculate: tool({
    description: 'Evaluate mathematical expressions',
    parameters: z.object({
      expression: z.string(),
    }),
    execute: async ({ expression }) => evaluate(expression),
  }),
  
  fetchData: tool({
    description: 'Retrieve data from database',
    parameters: z.object({
      query: z.string(),
      filters: z.array(z.string()),
    }),
    execute: async ({ query, filters }) => db.fetch(query, filters),
  }),
};
```

#### Using Tools with maxSteps
```typescript
const { text, steps } = await generateText({
  model: openai('gpt-4o'),
  tools,
  maxSteps: 5,
  prompt: 'Analyze this data and calculate results',
});

// Access all tool calls from steps
const allToolCalls = steps.flatMap(step => step.toolCalls);
```

## Anthropic Integration

### 1. Setup
```typescript
import { anthropic } from '@vercel/ai-sdk';

const model = anthropic('claude-3-opus-20240229');
```

### 2. Basic Text Generation
```typescript
const { text } = await generateText({
  model,
  prompt: 'Your prompt here',
  system: 'Optional system prompt',
});
```

### 3. Structured Output
```typescript
const { object } = await generateObject({
  model,
  schema: z.object({
    title: z.string(),
    summary: z.string(),
    keywords: z.array(z.string()),
  }),
  prompt: 'Analyze this content',
});
```

### 4. Streaming Responses
```typescript
const stream = await streamText({
  model,
  prompt: 'Generate a long response',
  onToken: (token) => {
    // Handle each token as it arrives
  },
});
```

## Best Practices

### 1. Error Handling
- Implement proper error boundaries
- Handle rate limits gracefully
- Provide fallback responses
- Log errors for debugging

### 2. Performance Optimization
- Use appropriate model sizes
- Implement caching strategies
- Optimize prompt length
- Monitor token usage

### 3. Security
- Protect API keys
- Validate user input
- Implement rate limiting
- Secure tool execution

### 4. Development Workflow
- Use TypeScript for type safety
- Test tool implementations
- Document prompt templates
- Monitor model performance

## Common Patterns

### 1. Chat Implementation
```typescript
const { messages, handleSubmit, handleInputChange } = useChat({
  api: '/api/chat',
  model: anthropic('claude-3-opus-20240229'),
  maxSteps: 3,
});
```

### 2. Tool Orchestration
```typescript
const agent = {
  tools,
  maxSteps: 5,
  onToolCall: async ({ toolCall }) => {
    // Handle tool execution
    return await executeTool(toolCall);
  },
};
```

### 3. Streaming with Tools
```typescript
const stream = await streamText({
  model,
  tools,
  maxSteps: 3,
  onToolCall: async (toolCall) => {
    // Handle streaming tool calls
    return await handleToolCall(toolCall);
  },
});
```

## Environment Setup
Required environment variables:
```env
ANTHROPIC_API_KEY=your_key_here
MODEL_NAME=claude-3-opus-20240229
```

## Development Guidelines
1. Start with simple implementations
2. Add tools incrementally
3. Test thoroughly
4. Monitor performance
5. Document changes
6. Handle edge cases 