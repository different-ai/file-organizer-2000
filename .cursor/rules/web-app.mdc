---
description: How to implement web app components and features
globs: packages/web/**/*
---

# Web Application Implementation Guide

## Project Structure
All web application code should be created in `packages/web/`

## Key Components

### 1. API Routes
- Place all API routes in `packages/web/api/`
- Use Next.js API routes pattern
- Implement rate limiting for public endpoints
- Include proper error handling

### 2. Authentication
- Implement using Next.js Auth.js
- Store user sessions in database
- Handle token refresh flows

### 3. Database Integration
- Use Prisma as ORM
- Keep schema in `packages/web/prisma/`
- Follow migration best practices

### 4. Frontend Components
- Use React Server Components where possible
- Implement proper loading states
- Follow accessibility guidelines

## Vercel AI SDK Integration

The next iteration of the chatbot will be implemented using Vercel AI SDK, with the frontend components integrated directly into the Obsidian plugin and the backend APIs hosted in the web application.

### Core Components

#### 1. Frontend Integration (Plugin)
```typescript
import { useChat } from '@ai-sdk/react'

export function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: '/api/chat', // Points to the web backend
    id: 'obsidian-chat',
    onResponse: (response) => {
      // Handle streaming responses
    },
    onFinish: (message) => {
      // Handle completion
    }
  })
}
```

#### 2. Backend API (Web)
```typescript
import { StreamingTextResponse, LangChainStream } from 'ai'
import { ChatOpenAI } from 'langchain/chat_models/openai'
import { AIMessage, HumanMessage } from 'langchain/schema'

// API route implementation
export async function POST(req: Request) {
  const { messages, context } = await req.json()
  const { stream, handlers } = LangChainStream()
  
  const llm = new ChatOpenAI({
    streaming: true,
    modelName: 'gpt-4-turbo-preview'
  })

  // Process messages with context
  llm.call(
    [
      ...context.map((c: string) => new SystemMessage(c)),
      ...messages.map((m: any) => 
        m.role === 'user' 
          ? new HumanMessage(m.content)
          : new AIMessage(m.content)
      )
    ],
    {},
    [handlers]
  )

  return new StreamingTextResponse(stream)
}
```

### Key Features

1. **Streaming Responses**
   - Real-time message streaming
   - Automatic UI updates
   - Progress indicators

2. **Context Management**
   - Maintain chat history
   - Include vault context
   - Handle file references

3. **Tool Integration**
   - File operations
   - Note creation/editing
   - Custom command execution

4. **Error Handling**
   - Graceful error recovery
   - User feedback
   - Automatic retries

### Advanced Features

1. **Message Persistence**
   - Local storage integration
   - Sync with backend
   - History management

2. **Multi-Modal Support**
   - Handle text, images, files
   - Process attachments
   - Support rich content

3. **Custom Tools**
   - Define custom actions
   - Handle tool callbacks
   - Manage tool state

### Implementation Notes

1. **Security**
   - API authentication
   - Rate limiting
   - Input validation

2. **Performance**
   - Message batching
   - Efficient streaming
   - Resource management

3. **Error Recovery**
   - Connection handling
   - State recovery
   - Fallback options

### API Route Implementation
```typescript
export async function POST(req: Request) {
  const { messages } = await req.json()
  const { stream, handlers } = LangChainStream()
  
  const llm = new ChatOpenAI({
    streaming: true,
    modelName: 'gpt-4-turbo-preview'
  })

  llm.call(
    messages.map((m: any) => 
      m.role === 'user' 
        ? new HumanMessage(m.content)
        : new AIMessage(m.content)
    ),
    {},
    [handlers]
  )

  return new StreamingTextResponse(stream)
}
```

### Frontend Usage
```typescript
'use client'
import { useChat } from 'ai/react'

export function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat()

  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>{m.content}</div>
      ))}
      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Say something..."
        />
      </form>
    </div>
  )
}
```

### Best Practices
1. Always implement proper error handling
2. Use streaming responses for better UX
3. Implement rate limiting
4. Cache responses when possible
5. Use environment variables for API keys
